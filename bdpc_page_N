# ‐*‐ coding: utf‐8 ‐*‐
"""
记录每个词前N的title、链接、排名位置
可以自由配置前几页
"""

import requests
from pyquery import PyQuery as pq
import threading
import queue
import time
from urllib.parse import urlparse
import time
import gc
import random
requests.packages.urllib3.disable_warnings()


cookie_str = """
BIDUPSID=F2515E4F29BB88B255962F2CFE19C3F9; PSTM=1646355832; __yjs_duid={0}; BD_UPN=12314353; BAIDUID=468546726793196028473969D2F035C8:SL=0:NR=10:FG=1; BDUSS=GZ0N0Z4MWNRVlBHZzRLblcwRjAtbGpXcVJzNUNBZFVIcDJoeE9uSGh4Y0FZeGxqRUFBQUFBJCQAAAAAAAAAAAEAAADag5oxzI2IkAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADW8WIA1vFiT; BDUSS_BFESS=GZ0N0Z4MWNRVlBHZzRLblcwRjAtbGpXcVJzNUNBZFVIcDJoeE9uSGh4Y0FZeGxqRUFBQUFBJCQAAAAAAAAAAAEAAADag5oxzI2IkAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADW8WIA1vFiT; BDORZ=B490B5EBF6F3CD402E515D22BCDA1598; H_WISE_SIDS=110085_180636_194529_196428_197471_204906_209568_210301_210322_211435_211985_212296_212874_213038_213346_214794_215730_216207_216714_216770_216844_216941_217084_217167_218548_219065_219743_219942_219946_220392_220639_220663_221121_221317_221409_221440_221468_221479_221502_221527_221697_221874_221921_222130_222299_222396_222618_222619_222625_222780_222862_223064_223192_223240_223253_223343_223374_223396_223476_223537_223595_223627_223765_223811_223824_223859_224014_224045_224055_224086_224098_224116_224141_224195_224428_224441_224632_224859_224915_225204_225245_225285_225341_225370_225409_225436_225485_225741_225765_225849; H_WISE_SIDS_BFESS=110085_180636_194529_196428_197471_204906_209568_210301_210322_211435_211985_212296_212874_213038_213346_214794_215730_216207_216714_216770_216844_216941_217084_217167_218548_219065_219743_219942_219946_220392_220639_220663_221121_221317_221409_221440_221468_221479_221502_221527_221697_221874_221921_222130_222299_222396_222618_222619_222625_222780_222862_223064_223192_223240_223253_223343_223374_223396_223476_223537_223595_223627_223765_223811_223824_223859_224014_224045_224055_224086_224098_224116_224141_224195_224428_224441_224632_224859_224915_225204_225245_225285_225341_225370_225409_225436_225485_225741_225765_225849; H_PS_PSSID=36552_36460_36641_37112_37139_36885_34813_36917_37003_37175_37135_26350_36862_22157; sug=3; sugstore=0; ORIGIN=2; bdime=20100; H_PS_645EC=d498njo38owD%2BjMNQRazj2tUTVAkJI%2B%2BO9pGUBTd4kA32odXVVtfB97kltoAvi5I6zNP; BAIDUID_BFESS=468546726793196028473969D2F035C8:SL=0:NR=10:FG=1
"""

# 生成随机cookie
def get_uid():
	seed = "1234567890ABCDEFGHIJKLMNOPQRSTUVWXYZ"
	lis = []
	[lis.append(random.choice(seed)) for _ in range(33)]
	uid = ''.join(lis)
	return uid


# 只解密加密url用
def get_header():
	my_header = {
		'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
		'Accept-Encoding': 'deflate',
		'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
		'Cache-Control': 'max-age=0',
		'Connection': 'keep-alive',
		'Cookie': cookie_str.strip().format(get_uid()),
		'Host': 'www.baidu.com',
		'Sec-Fetch-Dest': 'document',
		'Sec-Fetch-Mode': 'navigate',
		'Sec-Fetch-Site': 'same-origin',
		'Sec-Fetch-User': '?1',
		'Upgrade-Insecure-Requests': '1',
		'User-Agent':'Mozilla/5.0 (iPhone; CPU iPhone OS 13_2_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.3 Mobile/15E148 Safari/604.1',
		}
	return my_header


class bdpcCoverPage5(threading.Thread):

	def __init__(self):
		threading.Thread.__init__(self)

	@staticmethod
	def read_excel(filepath):
		q = queue.Queue()
		for kwd in open(filepath,'r',encoding='utf-8'):
			kwd = kwd.strip()
			if kwd:
				q.put(kwd)
		return q


	# 获取某词serp源码
	def get_html(self,url,retry=1):
		my_header = get_header()
		try:
			r = requests.get(url=url,headers=my_header,timeout=10)
		except Exception as e:
			print('获取源码失败',e)
			time.sleep(20)
			if retry > 0:
				self.get_html(url,retry-1)
		else:
			html = r.content.decode('utf-8',errors='ignore')  # 用r.text有时候识别错误
			url = r.url  # 反爬会重定向,取定向后的地址
			return html,url


	# 获取某词serp源码所有url
	def get_encrpt_urls(self, html, url):
		encrypt_url_list = []
		real_urls = []
		doc = pq(html)
		title = doc('title').text()
		if '_百度搜索' in title and 'https://www.baidu.com/' in url:
			div_list = doc('#content_left .result').items()  # 自然排名
			div_op_list = doc('#content_left .result-op').items()  # 非自然排名
			for div in div_list:
				rank = div.attr('id') if div.attr('id') else 'id_xxx'
				a = div('h3 a')
				if not a: # 空对象pyquery.pyquery.PyQuery 为假
					a = div('.c-result-content article section a')
				encrypt_url = a.attr('href')
				title = a.text().strip().replace('\n','')
				encrypt_url_list.append((encrypt_url, title,rank))
			for div in div_op_list:
				rank_op = div.attr('id') if div.attr('id') else 'id_xxx'
				link = div.attr('mu')  # 有些op样式没有mu属性
				if link:
					title = div('h3 a').text().strip().replace('\n','')
					real_urls.append((link, title,rank_op))
				else:
					a = div('article a')
					if a:
						encrypt_url = a.attr('href')
						title = a.text().strip().replace('\n','')
						real_urls.append((encrypt_url, title,rank_op))

		else:
			print('源码异常,可能反爬,暂停60 s', title)
			time.sleep(60)

		return encrypt_url_list, real_urls


	# 解密某条加密url
	def decrypt_url(self, encrypt_url, retry=1):
		if encrypt_url:
			my_header = get_header()
			try:
				encrypt_url = encrypt_url.replace('http://', 'https://') if 'https://' not in encrypt_url else encrypt_url
				r = requests.head(encrypt_url, headers=my_header,timeout=60)
			except Exception as e:
				print(encrypt_url, '解密失败,暂停45 s', e)
				time.sleep(45)
				if retry > 0:
					self.decrypt_url(encrypt_url, retry - 1)
			else:
				real_url = r.headers['Location'] if 'Location' in r.headers else None
				return real_url


	# 获取某词serp源码首页排名真实url
	def get_real_urls(self, encrypt_urls_rank):
		real_urls_rank = [(self.decrypt_url(encrypt_url),title,rank) for encrypt_url,title,rank in encrypt_urls_rank]
		return real_urls_rank


	# 线程函数
	def run(self):
		while 1:
			kwd = q.get()
			print(kwd)
			for page,page_num in page_dict.items():
				if page == '首页':
					url = f"https://www.baidu.com/s?tn=50000021_hao_pg&ie=utf-8&wd={kwd}"
				else:
					url = f"https://www.baidu.com/s?tn=50000021_hao_pg&ie=utf-8&wd={kwd}&pn={page_num}"
				# print(kwd,url)
				html_now_url = self.get_html(url)
				html,now_url = html_now_url if html_now_url else (None,None)
				if not html:
					continue
				encrypt_urls_rank,real_urls_op_rank = self.get_encrpt_urls(html,now_url)
				if encrypt_urls_rank:
					real_urls_rank = self.get_real_urls(encrypt_urls_rank)
					real_urls_rank.extend(real_urls_op_rank)
					for url,title,rank in real_urls_rank:
						lock.acquire()
						f.write(f'{today}\t{kwd}\t{page}\t{title}\t{url}\t{rank}\n')
						lock.release()
			f.flush()
			q.task_done()
			del kwd
			gc.collect()


if __name__ == "__main__":
	start = time.time()
	local_time = time.localtime()
	today = time.strftime('%Y-%m-%d',local_time)
	q = bdpcCoverPage5.read_excel('kwd.txt')  # 关键词队列
	page_dict = {'首页':'','二页':10,'三页':20,'四页':30,'五页':40}  # 查询页码
	lock = threading.Lock()
	f = open(f'./记录/{today}_bdpc1_page3_info.txt','w',encoding="utf-8")
	# 设置线程数
	for i in list(range(1)):
		t = bdpcCoverPage5()
		t.setDaemon(True)
		t.start()
	q.join()
	f.close()
	end = time.time()
	print('耗时{0}min'.format((end - start) / 60))

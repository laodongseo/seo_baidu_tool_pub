# ‐*‐ coding: utf‐8 ‐*‐
"""
一个关键词serp上同一个域名出现N个url排名 计算N次
相关网站.相关企业.智能小程序.其他人还在搜都会包含
热议聚合.资讯聚合包含.搜索智能聚合包含.视频包含(黄忠小区二手房)
百度百科不含,百度手机助手下载不含
sigma.baidu.com == xx_相关网站|xx_相关企业
recommend_list.baidu.com == 其他人还在搜
nourl.ubs.baidu.com == 搜索智能聚合
bzclk.baidu.com == 结构化的展示样式
准备kwd.txt
"""
import requests
from pyquery import PyQuery as pq
import threading
import queue
import time
from urllib.parse import urlparse
import gc
import json
from openpyxl import load_workbook
from openpyxl import Workbook





# 提取某条url域名部分
def get_domain(real_url):

    # 通过mu提取url有些非自然排名url是空
    try:
       res = urlparse(real_url)  # real_url为空不会报错
    except Exception as e:
       print (e,real_url)
       domain = "xxx"
    else:
       domain = res.netloc
    return domain


# 获取某词serp源码首页排名真实url的域名部分
def get_domains(real_urls):
        domain_list = [get_domain(real_url) for real_url in real_urls]
        # 搜一个词 同一个域名多个url出现排名 只计算一次
        return domain_list

def save():
    res_format = result.items()
    #写入excel文件
    wb = Workbook()
    # 创建sheet
    for city in city_list:
        sheet_num = 0
        wb.create_sheet(u'{0}'.format(city),index=sheet_num)
        sheet_num += 1
    for city,data_dict in res_format:
        sort_dict = sorted(data_dict.items(), key=lambda s: s[1], reverse=True)
        for domain,num in sort_dict:
            row_value = [domain,num]
            wb[u'{0}'.format(city)].append(row_value)
    wb.save('{0}bdmo1_page5.xlsx'.format(today))

    # 写入txt
    res_format = sorted(result_all.items(), key=lambda s: s[1], reverse=True)
    with open('res.txt','w',encoding='utf-8') as f:
        for domain,num in res_format:
            f.write(domain+'\t'+str(num)+'\n')

class bdmoCover(threading.Thread):

    def __init__(self):
        threading.Thread.__init__(self)

    # 读取文件 关键词进入队列
    @staticmethod
    def read_excel(filepath):
        q = queue.Queue()
        city_list = []
        wb_kwd = load_workbook(filepath)
        for sheet_obj in wb_kwd:
            sheet_name = sheet_obj.title
            city_list.append(sheet_name)
            col_a = sheet_obj['A']
            for cell in col_a:
                kwd = (cell.value)
                # 加个判断,防止一些不可见字符
                if kwd:
                    kwd_z = kwd + '租房'
                    kwd_er = kwd + '二手房'
                    q.put([sheet_name,kwd_z])
                    q.put([sheet_name,kwd_er])
        return q,city_list

    # 初始化结果字典
    @staticmethod
    def result_init(group_list):
        result = {}
        for group in group_list:
            result[group] = {}
        print("结果字典init...")
        return result

    # 获取某词serp源码
    def get_html(self,url,retry=2):
        try:
            r = requests.get(url=url,headers=user_agent,timeout=5)
        except Exception as e:
            print('获取源码失败',e)
            if retry > 0:
                self.get_html(url,retry-1)
        else:
            html = r.text
            return html

    # 获取某词的serp源码上包含排名url的div块
    def get_data_logs(self, html):
        data_logs = []
        url_other = []
        if html and '百度' in html:
            doc = pq(html)
            try:
                div_list = doc('.c-result').items()
                # 如果mu为空,.c-result-content header a会有数据,否则没有
                a_list = doc('.c-result .c-result-content header a').items()
            except Exception as e:
                print('提取div块失败', e)
            else:
                for div in div_list:
                    data_log = div.attr('data-log')
                    data_logs.append(data_log) if data_log is not None else data_logs
                for a in a_list:
                    href = a.attr('data-sf-href')
                    url_other.append(href) if href is not None else href
        else:
            print('源码异常---------------------')
            time.sleep(6)
        return data_logs,url_other

    # 提取排名的真实url
    def get_real_urls(self, data_logs=[]):
        real_urls = []
        for data_log in data_logs:
            # json字符串要以双引号表示
            data_log = json.loads(data_log.replace("'", '"'))
            # 如果mu是空的话,real_urls里面会有空元素
            url = data_log['mu']
            real_urls.append(url)
        return real_urls


    # 线程函数
    def run(self):
        while 1:
            city,kwd = q.get()
            # kwd = '王府井二手房'
            print(city,kwd,q.qsize())
            url = "https://m.baidu.com/s?ie=utf-8&word={0}".format(kwd)
            try:
                html = self.get_html(url)
                # f_url.write(html)
                data_logs,url_other = self.get_data_logs(html)
                real_urls = self.get_real_urls(data_logs)
                real_urls.extend(url_other)
                real_urls = [i for i in real_urls if i != '']
                for url_serp in real_urls:
                    f_url.write(kwd+'\t'+url_serp + '\t' + city + '\n')
                f_url.flush()
                del kwd
                gc.collect()
            except Exception as e:
                print(e)
            finally:
                q.task_done()



if __name__ == "__main__":
    start = time.time()
    local_time = time.localtime()
    today = time.strftime('%Y-%m-%d', local_time)
    f_url = open('url_serp{}.txt'.format(today),'w',encoding="utf-8")
    q,city_list = bdmoCover.read_excel('kwd_core_city.xlsx')
    result = bdmoCover.result_init(city_list)  # 结果字典
    result_all={}
    # user_agent = {
    #     'User-Agent': 'Mozilla/5.0 (Linux; Android 8.1.0; ALP-AL00 Build/HUAWEIALP-AL00; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/63.0.3239.83 Mobile Safari/537.36 T7/10.13 baiduboxapp/10.13.0.11 (Baidu; P1 8.1.0)'}
    user_agent = {
        'User-Agent': 'Mozilla/5.0 (Linux; Android 9; GM1900 Build/PKQ1.190110.001; wv) AppleWebKit/537.36 (KHTML, like Gecko) Version/4.0 Chrome/66.0.3359.158 Mobile Safari/537.36 MicroMessenger/7.0.6.1500(0x2700063E) Process/appbrand0 NetType/4G Language/zh_CN'}
    # user_agent = {
    #     'User-Agent': 'Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Mobile Safari/537.36'}
    # user_agent = {
    # 'User-Agent':'Mozilla/5.0 (iPhone; CPU iPhone OS 12_1_4 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.0 Mobile/15E148 Safari/604.1'
    # }
    #设置线程数
    for i in list(range(6)):
        t = bdmoCover()
        t.setDaemon(True)
        t.start()
    q.join()
    f_url.close()

    for i in open('url_serp{}.txt'.format(today),'r',encoding='utf-8'):
        i = i.strip()
        line = i.split('\t')
        url = line[1]
        city = line[2]
        if url.startswith('http'):
            domain = get_domain(url)
            result[city][domain] = result[city][domain]+1 if domain in result[city] else 1
            result_all[domain] = result_all[domain]+1 if domain in result_all else 1
        if url.startswith('/sf'):
            result[city]['/sf'] = result[city]['/sf'] + 1 if '/sf' in result[city] else 1
            result_all['/sf'] = result_all['/sf'] + 1 if '/sf' in result_all else 1
    # 结果保存文件
    save()

    end = time.time()

# ‐*‐ coding: utf‐8 ‐*‐
"""
准备domain.txt,一行一个域名,必须带http和https
区分https或者http，不区分 http://aaa/和http://aaa
查询site:domain看domain是否在serp首页，再访问该domain获取title，通过搜索title看该domain是否有排名
"""

import requests
from pyquery import PyQuery as pq
import threading
import queue


class BdpcJquan(threading.Thread):

    def __init__(self):
        threading.Thread.__init__(self)

    # 读取txt文件 获取待查询域名
    @staticmethod
    def read_txt(filepath):
        q = queue.Queue()
        for url in open(filepath, encoding='utf-8'):
            url = url.strip()
            q.put(url)
        return q

    # 获取某待查询域名的title
    def get_title(self, url,retry=1):
        title = '...'
        try:
            r = requests.get(url=url, headers=user_agent, timeout=5)
        except Exception as e:
            print('获取源码失败', url, e)
            if retry > 0:
                self.get_html(url, retry - 1)
        else:
            # r.encoding = 'gbk'
            html = r.text
            if html:
                try:
                    doc = pq(html)
                    title = doc('title').text()
                except Exception as e:
                    print(e)
                finally:
                    pass
            return title

    # 获取某待查询url或某词的serp源码
    def get_html(self, url, retry=2):
        try:
            r = requests.get(url=url, headers=user_agent, timeout=5)
        except Exception as e:
            print('获取源码失败', url, e)
            if retry > 0:
                self.get_html(url, retry - 1)
        else:
            # r.encoding = 'utf-8'
            html = r.text
            return html

    # 获取某待查询url或某词的serp源码上10条加密url
    def get_encrpt_urls(self, html):
        encrypt_url_list = []
        if html and '_百度搜索' in html:
            doc = pq(html)
            try:
                a_list = doc('.t a').items()
            except Exception as e:
                print('未找到加密url，站点未收录', e)
            else:
                for a in a_list:
                    encrypt_url = a.attr('href')
                    if encrypt_url.find('http://www.baidu.com/link?url=') == 0:
                        encrypt_url_list.append(encrypt_url)
        return encrypt_url_list

    # 解密某条加密url
    def decrypt_url(self, encrypt_url, retry=1):
        try:
            encrypt_url = encrypt_url.replace('http://', 'https://')
            r = requests.head(encrypt_url, headers=user_agent)
        except Exception as e:
            print(encrypt_url, '解密失败', e)
            if retry > 0:
                self.decrypt_url(encrypt_url, retry - 1)
        else:
            return r.headers['Location']

    # 获取某待查询url或某词的serp源码首页真实url
    def get_real_urls(self, encrypt_url_list):
        if encrypt_url_list:
            real_url_list = [self.decrypt_url(encrypt_url) for encrypt_url in encrypt_url_list]
            return real_url_list
        else:
            return []

    # 检查链接是否有site在首页
    def check_include(self, url, real_urls):
        real_urls_str = ''.join(real_urls)
        if url in real_urls_str:
            return 1
        else:
            return 0

    # 线程函数
    def run(self):
        while 1:
            domain = q.get()
            # print('查询', domain)
            # 查询该域名site是否在首页
            domain_site = domain.split('://')[1]
            url = "https://www.baidu.com/s?ie=utf-8&wd={0}".format('site:'+domain_site)
            print(url)
            html = self.get_html(url)
            encrypt_url_list = self.get_encrpt_urls(html)
            real_urls = self.get_real_urls(encrypt_url_list)
            num_domain = self.check_include(domain, real_urls)
            # 判断是否索引
            if num_domain == 1:
                # 查询该domain的title 检查是否有索引
                title = self.get_title(domain)
                # print(domain,title)
                url = "https://www.baidu.com/s?ie=utf-8&wd={0}".format(title)
                html = self.get_html(url)
                encrypt_url_list = self.get_encrpt_urls(html)
                real_urls = self.get_real_urls(encrypt_url_list)
                num_title = self.check_include(domain, real_urls)
                if num_title == 1:
                    # print("site在首页且索引")
                    f.write(domain+'\t'+'site在首页且索引\n')
                elif num_title == 0:
                    # print("site在首页无索引")
                    f.write(domain + '\t' + 'site在首页无索引\n')
            elif num_domain == 0:
                # print("疑似降权")
                f.write(domain + '\t' + '疑似降权\n')
            f.flush()
            q.task_done()


if __name__ == "__main__":

    user_agent = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36'}
    q = BdpcJquan.read_txt('links.txt')
    f = open('bdpc_links_res.txt','w',encoding='utf-8')
    # 设置线程数
    for i in list(range(1)):
        t = BdpcJquan()
        t.setDaemon(True)
        t.start()
    q.join()
    f.flush()
    f.close()

